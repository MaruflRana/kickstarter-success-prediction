{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kickstarter Success Prediction with SMOTE, Pipelines, Logistic Regression & ANN\n",
    "\n",
    "This notebook demonstrates a full ML workflow for Kickstarter campaign success prediction using both classical and deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1️⃣ Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣ Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2️⃣ Load Data\n",
    "data = pd.read_csv(r'E:\\ML_Project\\kickstarter-success-prediction\\data\\ks-projects-201801.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Initial Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3️⃣ Initial Cleaning\n",
    "data = data.drop(['ID', 'name'], axis=1)\n",
    "\n",
    "# Drop rows where state is not failed or successful\n",
    "data = data[data['state'].isin(['failed', 'successful'])].reset_index(drop=True)\n",
    "\n",
    "# Drop leakage columns — info only available after campaign ends\n",
    "leakage_cols = ['pledged', 'usd pledged', 'usd_pledged_real', 'backers']\n",
    "data = data.drop(leakage_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4️⃣ Feature Engineering\n",
    "\n",
    "# Parse dates\n",
    "data['launched_dt'] = pd.to_datetime(data['launched'])\n",
    "data['deadline_dt'] = pd.to_datetime(data['deadline'])\n",
    "\n",
    "# Campaign duration in days\n",
    "data['duration_days'] = (data['deadline_dt'] - data['launched_dt']).dt.days\n",
    "\n",
    "# Launch day of week and hour\n",
    "data['launch_dayofweek'] = data['launched_dt'].dt.dayofweek\n",
    "data['launch_hour'] = data['launched_dt'].dt.hour\n",
    "\n",
    "# Drop original datetime columns\n",
    "data = data.drop(['launched', 'deadline', 'launched_dt', 'deadline_dt'], axis=1)\n",
    "\n",
    "# Encode target\n",
    "data['state'] = data['state'].apply(lambda x: 1 if x == 'successful' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ Separate Features & Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5️⃣ Separate Features & Target\n",
    "X = data.drop('state', axis=1)\n",
    "y = data['state']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6️⃣ Identify categorical and numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6️⃣ Identify categorical and numeric columns\n",
    "cat_cols = ['category', 'main_category', 'currency', 'country']\n",
    "num_cols = [col for col in X.columns if col not in cat_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7️⃣ Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7️⃣ Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.7, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8️⃣ Preprocessing Pipeline (OneHot + Scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8️⃣ Preprocessing Pipeline (OneHot + Scaling)\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), num_cols),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse=False), cat_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform train data\n",
    "X_train_preproc = preprocessor.fit_transform(X_train)\n",
    "X_test_preproc = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9️⃣ Apply SMOTE to balance classes after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9️⃣ Apply SMOTE to balance classes after preprocessing\n",
    "# We create a pipeline without oversampling first just to transform features for SMOTE\n",
    "\n",
    "# SMOTE (oversample minority class in training set)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_bal, y_train_bal = smote.fit_resample(X_train_preproc, y_train)\n",
    "\n",
    "print(f'Before SMOTE: {np.bincount(y_train)}')\n",
    "print(f'After SMOTE: {np.bincount(y_train_bal)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10️⃣ Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10️⃣ Logistic Regression Model\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=500, random_state=42)\n",
    "lr_model.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "# Predict and Evaluate Logistic Regression\n",
    "y_pred_lr = lr_model.predict(X_test_preproc)\n",
    "y_pred_prob_lr = lr_model.predict_proba(X_test_preproc)[:,1]\n",
    "\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Logistic Regression Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve LR\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_pred_prob_lr)\n",
    "roc_auc_lr = auc(fpr_lr, tpr_lr)\n",
    "\n",
    "plt.plot(fpr_lr, tpr_lr, label=f'LogReg AUC = {roc_auc_lr:.3f}')\n",
    "plt.plot([0,1],[0,1],'--', color='gray')\n",
    "plt.title('Logistic Regression ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11️⃣ Build and Train ANN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11️⃣ Build and Train ANN Model\n",
    "\n",
    "input_shape = X_train_bal.shape[1]\n",
    "\n",
    "ann_model = models.Sequential([\n",
    "    layers.InputLayer(input_shape=(input_shape,)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "ann_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = ann_model.fit(\n",
    "    X_train_bal,\n",
    "    y_train_bal,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12️⃣ ANN Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12️⃣ ANN Evaluate on Test Set\n",
    "\n",
    "y_pred_prob_ann = ann_model.predict(X_test_preproc).ravel()\n",
    "y_pred_ann = (y_pred_prob_ann > 0.5).astype(int)\n",
    "\n",
    "print(\"ANN Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_ann))\n",
    "\n",
    "cm_ann = confusion_matrix(y_test, y_pred_ann)\n",
    "sns.heatmap(cm_ann, annot=True, fmt='d', cmap='Greens')\n",
    "plt.title('ANN Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve ANN\n",
    "fpr_ann, tpr_ann, _ = roc_curve(y_test, y_pred_prob_ann)\n",
    "roc_auc_ann = auc(fpr_ann, tpr_ann)\n",
    "\n",
    "plt.plot(fpr_ann, tpr_ann, label=f'ANN AUC = {roc_auc_ann:.3f}', color='green')\n",
    "plt.plot(fpr_lr, tpr_lr, label=f'LogReg AUC = {roc_auc_lr:.3f}', color='blue', linestyle='--')\n",
    "plt.plot([0,1],[0,1],'--', color='gray')\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13️⃣ Plot ANN Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13️⃣ Plot ANN Training History\n",
    "pd.DataFrame(history.history)[['loss','val_loss']].plot(title='ANN Loss')\n",
    "plt.show()\n",
    "\n",
    "pd.DataFrame(history.history)[['accuracy','val_accuracy']].plot(title='ANN Accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "papermill": {
   "duration": 207.587453,
   "end_time": "2020-10-18T19:17:00.806490",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-18T19:13:33.219037",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
